{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsHqbEo0ldgZ"
      },
      "source": [
        "## 🪄 Install `raga-testing-platform` library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mptHVuOXldge",
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6be4bdc2-f640-4a84-e7f1-d464f6c3347c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple\n",
            "Collecting raga-testing-platform==1.2.3\n",
            "  Downloading https://test-files.pythonhosted.org/packages/1c/8e/e80a42512e2d447060704c3d288687db6c8235b472a624e51fd65fc0f27d/raga_testing_platform-1.2.3-py3-none-any.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.0.2 (from raga-testing-platform==1.2.3)\n",
            "  Downloading pandas-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3==1.26.7 (from raga-testing-platform==1.2.3)\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from raga-testing-platform==1.2.3) (2.31.0)\n",
            "Collecting numpy==1.24.3 (from raga-testing-platform==1.2.3)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting halo==0.0.31 (from raga-testing-platform==1.2.3)\n",
            "  Downloading halo-0.0.31.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3==1.28.52 (from raga-testing-platform==1.2.3)\n",
            "  Downloading boto3-1.28.52-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker==7.0.0 (from raga-testing-platform==1.2.3)\n",
            "  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain==0.1.13 (from raga-testing-platform==1.2.3)\n",
            "  Downloading langchain-0.1.13-py3-none-any.whl (810 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-community==0.0.30 (from raga-testing-platform==1.2.3)\n",
            "  Downloading langchain_community-0.0.30-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai==0.1.1 (from raga-testing-platform==1.2.3)\n",
            "  Downloading langchain_openai-0.1.1-py3-none-any.whl (32 kB)\n",
            "Collecting openai==1.14.3 (from raga-testing-platform==1.2.3)\n",
            "  Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api==1.23.0 (from raga-testing-platform==1.2.3)\n",
            "  Downloading opentelemetry_api-1.23.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openinference-instrumentation-langchain==0.1.14 (from raga-testing-platform==1.2.3)\n",
            "  Downloading openinference_instrumentation_langchain-0.1.14-py3-none-any.whl (13 kB)\n",
            "Collecting openinference-instrumentation-openai==0.1.4 (from raga-testing-platform==1.2.3)\n",
            "  Downloading openinference_instrumentation_openai-0.1.4-py3-none-any.whl (21 kB)\n",
            "Collecting opentelemetry-sdk==1.23.0 (from raga-testing-platform==1.2.3)\n",
            "  Downloading opentelemetry_sdk-1.23.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.32.0,>=1.31.52 (from boto3==1.28.52->raga-testing-platform==1.2.3)\n",
            "  Downloading botocore-1.31.85-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3==1.28.52->raga-testing-platform==1.2.3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3==1.28.52->raga-testing-platform==1.2.3)\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker==7.0.0->raga-testing-platform==1.2.3) (24.0)\n",
            "Collecting log_symbols>=0.0.14 (from halo==0.0.31->raga-testing-platform==1.2.3)\n",
            "  Downloading log_symbols-0.0.14-py3-none-any.whl (3.1 kB)\n",
            "Collecting spinners>=0.0.24 (from halo==0.0.31->raga-testing-platform==1.2.3)\n",
            "  Downloading spinners-0.0.24-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from halo==0.0.31->raga-testing-platform==1.2.3) (2.4.0)\n",
            "Collecting colorama>=0.3.9 (from halo==0.0.31->raga-testing-platform==1.2.3)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from halo==0.0.31->raga-testing-platform==1.2.3) (1.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.13->raga-testing-platform==1.2.3) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.13->raga-testing-platform==1.2.3) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.13->raga-testing-platform==1.2.3) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.13->raga-testing-platform==1.2.3) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.33 (from langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading langsmith-0.1.59-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.13->raga-testing-platform==1.2.3) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.13->raga-testing-platform==1.2.3) (8.3.0)\n",
            "Collecting tiktoken<1,>=0.5.2 (from langchain-openai==0.1.1->raga-testing-platform==1.2.3)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3->raga-testing-platform==1.2.3) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.14.3->raga-testing-platform==1.2.3) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.14.3->raga-testing-platform==1.2.3)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3->raga-testing-platform==1.2.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3->raga-testing-platform==1.2.3) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3->raga-testing-platform==1.2.3) (4.11.0)\n",
            "Collecting openinference-semantic-conventions>=0.1.3 (from openinference-instrumentation-langchain==0.1.14->raga-testing-platform==1.2.3)\n",
            "  Downloading openinference_semantic_conventions-0.1.6-py3-none-any.whl (8.5 kB)\n",
            "Collecting opentelemetry-instrumentation (from openinference-instrumentation-langchain==0.1.14->raga-testing-platform==1.2.3)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions (from openinference-instrumentation-langchain==0.1.14->raga-testing-platform==1.2.3)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-langchain==0.1.14->raga-testing-platform==1.2.3) (1.14.1)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api==1.23.0->raga-testing-platform==1.2.3)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api==1.23.0->raga-testing-platform==1.2.3)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting opentelemetry-semantic-conventions (from openinference-instrumentation-langchain==0.1.14->raga-testing-platform==1.2.3)\n",
            "  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.2->raga-testing-platform==1.2.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.2->raga-testing-platform==1.2.3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.2->raga-testing-platform==1.2.3) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->raga-testing-platform==1.2.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->raga-testing-platform==1.2.3) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->raga-testing-platform==1.2.3) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13->raga-testing-platform==1.2.3) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13->raga-testing-platform==1.2.3) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13->raga-testing-platform==1.2.3) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13->raga-testing-platform==1.2.3) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13->raga-testing-platform==1.2.3) (1.9.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.14.3->raga-testing-platform==1.2.3) (1.2.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.14.3->raga-testing-platform==1.2.3)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.14.3->raga-testing-platform==1.2.3)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api==1.23.0->raga-testing-platform==1.2.3) (3.18.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging>=14.0 (from docker==7.0.0->raga-testing-platform==1.2.3)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.13->raga-testing-platform==1.2.3) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.13->raga-testing-platform==1.2.3) (2.18.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.13->raga-testing-platform==1.2.3) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.1->raga-testing-platform==1.2.3) (2023.12.25)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation->openinference-instrumentation-langchain==0.1.14->raga-testing-platform==1.2.3) (67.7.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.13->raga-testing-platform==1.2.3)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: halo\n",
            "  Building wheel for halo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for halo: filename=halo-0.0.31-py3-none-any.whl size=11232 sha256=748ae19e895e737cc3646b71755b4b77fa3aa45077e773ac228f98c4386d6532\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d9/8a/b4f14c44aba7c164d4379eca6f1dde59360050406b1edaec24\n",
            "Successfully built halo\n",
            "Installing collected packages: spinners, urllib3, packaging, orjson, opentelemetry-semantic-conventions, openinference-semantic-conventions, numpy, mypy-extensions, jsonpointer, jmespath, importlib-metadata, h11, deprecated, colorama, typing-inspect, pandas, opentelemetry-api, marshmallow, log_symbols, jsonpatch, httpcore, botocore, tiktoken, s3transfer, opentelemetry-sdk, opentelemetry-instrumentation, langsmith, httpx, halo, docker, dataclasses-json, openinference-instrumentation-openai, openinference-instrumentation-langchain, openai, langchain-core, boto3, langchain-text-splitters, langchain-openai, langchain-community, langchain, raga-testing-platform\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.0.2 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.28.52 botocore-1.31.85 colorama-0.4.6 dataclasses-json-0.6.6 deprecated-1.2.14 docker-7.0.0 h11-0.14.0 halo-0.0.31 httpcore-1.0.5 httpx-0.27.0 importlib-metadata-6.11.0 jmespath-1.0.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.13 langchain-community-0.0.30 langchain-core-0.1.52 langchain-openai-0.1.1 langchain-text-splitters-0.0.2 langsmith-0.1.59 log_symbols-0.0.14 marshmallow-3.21.2 mypy-extensions-1.0.0 numpy-1.24.3 openai-1.14.3 openinference-instrumentation-langchain-0.1.14 openinference-instrumentation-openai-0.1.4 openinference-semantic-conventions-0.1.6 opentelemetry-api-1.23.0 opentelemetry-instrumentation-0.45b0 opentelemetry-sdk-1.23.0 opentelemetry-semantic-conventions-0.44b0 orjson-3.10.3 packaging-23.2 pandas-2.0.2 raga-testing-platform-1.2.3 s3transfer-0.6.2 spinners-0.0.24 tiktoken-0.7.0 typing-inspect-0.9.0 urllib3-1.26.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "ce0b1bcbd20f44a8990d5d1c49cc6802"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple raga-testing-platform==1.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqvpwqSldgg"
      },
      "source": [
        "### Put your `ACCESS KEY & SECRET KEY`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX8PY6OOldgg"
      },
      "outputs": [],
      "source": [
        "# Define the access key, secret key, and host\n",
        "ACCESS_KEY = \"VibRu71BAYZME5rOqZui\"\n",
        "SECRET_KEY = \"DLiKgCoEjOlVJ6dyieHrDPbDKIMhYBZEZWVqaNM5\"\n",
        "HOST = \"https://backend.platform.raga.ai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOPfGqW7ldgh"
      },
      "source": [
        "### Import All raga lib from raga module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO827YR-ldgh"
      },
      "outputs": [],
      "source": [
        "from raga import *\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Image Recognition**\n",
        "\n",
        "--------"
      ],
      "metadata": {
        "id": "R0ficl0sDVuy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpo6WmgOldgh"
      },
      "source": [
        "### 🪄 Set up a Run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ul7jkrCCldgi"
      },
      "outputs": [],
      "source": [
        "run_name = \"Image recognition test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxzm7eKbldgk"
      },
      "source": [
        "### Active Learning Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXRzPXgxldgk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "dataset_name = \"image_classification_train\"\n",
        "budget = 6000\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "edge_case_detection = active_learning(test_session=test_session,\n",
        "                                      dataset_name = dataset_name,\n",
        "                                      test_name = \"active_learniing1\",\n",
        "                                      type = \"active_learning\",\n",
        "                                      output_type=\"curated_dataset\",\n",
        "                                      embed_col_name=\"ImageVectorsM1\",\n",
        "                                      budget=budget)\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Drift Test"
      ],
      "metadata": {
        "id": "9DrP4RDLfjOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_dataset_name = \"image_classification_train\"\n",
        "eval_dataset_name = \"image_classification_test\"\n",
        "threshold = 79\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = DriftDetectionRules()\n",
        "rules.add(type=\"single_class_anomaly_detection\", dist_metric=\"Mahalanobis\", _class=\"ALL\", threshold=threshold)\n",
        "\n",
        "#To Run OD Test\n",
        "edge_case_detection = data_drift_detection(test_session=test_session,\n",
        "                                           test_name=\"drift_test3\",\n",
        "                                           train_dataset_name=reference_dataset_name,\n",
        "                                           field_dataset_name=eval_dataset_name,\n",
        "                                           train_embed_col_name=\"ImageVectorsM1\",\n",
        "                                           field_embed_col_name = \"ImageVectorsM1\",\n",
        "                                           level = \"image\",\n",
        "                                           rules = rules)\n",
        "\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "YqOZJcTmfi9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD7C7PUUldgl"
      },
      "source": [
        "### Class Imbalance Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkcQ0sfkldgl"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"image_classification_train\"\n",
        "metric_threshold = 0.99\n",
        "rules = ClassImbalanceRules()\n",
        "\n",
        "rules.add(metric=\"js_divergence\", ideal_distribution=\"uniform\", metric_threshold=metric_threshold, label=\"ALL\")\n",
        "rules.add(metric=\"chi_squared_test\", ideal_distribution=\"uniform\", metric_threshold=metric_threshold, label=\"ALL\")\n",
        "\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "distribution_test = class_imbalance_test(test_session=test_session,\n",
        "                                         dataset_name=dataset_name,\n",
        "                                         test_name=\"Class Imbalance Test\",\n",
        "                                         type=\"class_imbalance\",\n",
        "                                         output_type=\"image_classification\", #\"object_detection\",\n",
        "                                         annotation_column_name=\"GroundTruth\",\n",
        "                                         rules=rules)\n",
        "\n",
        "test_session.add(distribution_test)\n",
        "test_session.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Near Duplicates Test"
      ],
      "metadata": {
        "id": "4vNNh2vtuG86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"image_classification_train\"\n",
        "metric_threshold = 0.99\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = LQRules()\n",
        "rules.add(metric=\"similarity_score\", metric_threshold=metric_threshold)\n",
        "\n",
        "\n",
        "\n",
        "edge_case_detection = nearest_duplicate(test_session=test_session,\n",
        "                                          dataset_name = dataset_name,\n",
        "                                          test_name = \"Near-Duplicates-v4\",\n",
        "                                          type = \"near_duplicates\",\n",
        "                                          output_type=\"near_duplicates\",\n",
        "                                          embed_col_name=\"ImageVectorsM1\",\n",
        "                                          rules=rules)\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "hyZxAS6FuGvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier Detection Test"
      ],
      "metadata": {
        "id": "GVAn448WtLvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold=60\n",
        "dataset_name = \"after_activelearning_dataset\"\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = DriftDetectionRules()\n",
        "rules.add(type=\"anomaly_detection\", dist_metric=\"Mahalanobis\", _class=\"ALL\", threshold=threshold)\n",
        "edge_case_detection = data_drift_detection(test_session=test_session,\n",
        "                                           test_name=\"Outlier-detection-train-after_active_learning\",\n",
        "                                           dataset_name=dataset_name,\n",
        "                                           embed_col_name = \"ImageVectorsM1\",\n",
        "                                        #    output_type = \"image_classification\",\n",
        "                                           output_type = \"outlier_detection\",\n",
        "                                           rules = rules)\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "fFffljMAtLgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Leakage Test"
      ],
      "metadata": {
        "id": "MseyvRXtgR_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_dataset_name = \"image_classification_train\"\n",
        "eval_dataset_name = \"image_classification_test\"\n",
        "\n",
        "metric_threshold = 0.9\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = LQRules()\n",
        "rules.add(metric = 'overlapping_samples', metric_threshold = metric_threshold)\n",
        "\n",
        "\n",
        "edge_case_detection = data_leakage_test(test_session=test_session,\n",
        "                                           test_name=\"dataleakage1\",\n",
        "                                           train_dataset_name=reference_dataset_name,\n",
        "                                           dataset_name=eval_dataset_name,\n",
        "                                           type = \"data_leakage\",\n",
        "                                           output_type=\"image_data\",\n",
        "                                           train_embed_col_name=\"ImageVectorsM1\",\n",
        "                                           embed_col_name = \"ImageVectorsM1\",\n",
        "                                           rules = rules)\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "BkpSSFx0gRwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IrrwMUhldgp"
      },
      "source": [
        "### Labeling Quality Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxhJnOcJldgp"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'image_classification_lqc_test_after_deduplication'\n",
        "metric_threshold = 0.321\n",
        "\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "\n",
        "rules = LQRules()\n",
        "rules.add(metric=\"mistake_score\", label=[\"ALL\"], metric_threshold=metric_threshold)\n",
        "edge_case_detection = labelling_quality_test(\n",
        "    test_session=test_session,\n",
        "    dataset_name=dataset_name,\n",
        "    test_name=\"IC- Labeling Quality Test\",\n",
        "    type=\"labelling_consistency\",\n",
        "    output_type=\"image_classification\",\n",
        "    mistake_score_col_name=\"MistakeScore\",\n",
        "    embedding_col_name=\"ImageVectorsM1\",\n",
        "    rules=rules,\n",
        ")\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "test_session.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56LQV5UAp3Uz"
      },
      "source": [
        "###Image Property Drift Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTqHSw-Ep3DX"
      },
      "outputs": [],
      "source": [
        "reference_dataset_name = \"image_classification_train\"\n",
        "eval_dataset_name = \"image_classification_test\"\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "\n",
        "rules = IPDRules()\n",
        "rules.add(metric=\"image-property-suite\")\n",
        "\n",
        "edge_case_detection = image_property_drift(test_session=test_session,\n",
        "                                           reference_dataset_name=reference_dataset_name,\n",
        "                                           eval_dataset_name=eval_dataset_name,\n",
        "                                           rules=rules,\n",
        "                                           test_name=\"Image-Property-Drift\",\n",
        "                                           type=\"image-property-drift\",\n",
        "                                           output_type=\"image-data\"\n",
        "                                           )\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Image Detection**"
      ],
      "metadata": {
        "id": "t5p9UTO8DuRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🪄 Set up a Run"
      ],
      "metadata": {
        "id": "MudhhNDPEJ9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_name = \"Image detection test\""
      ],
      "metadata": {
        "id": "FFNd1va_Dxyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Active Learning Test\n"
      ],
      "metadata": {
        "id": "sTtCTmoqENYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"object_detection_al_lq_v2\"\n",
        "budget = 6000\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "edge_case_detection = active_learning(test_session=test_session,\n",
        "                                      dataset_name = dataset_name,\n",
        "                                      test_name = \"active-learning-test-2\",\n",
        "                                      type = \"active_learning\",\n",
        "                                      output_type=\"curated_dataset\",\n",
        "                                      embed_col_name=\"ImageVectorsM1\",\n",
        "                                      budget=budget)\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "6Frxj2dZEQaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Drift Test"
      ],
      "metadata": {
        "id": "TTj5Xh0_ETWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_name = \"od_train_after_deduplication\"\n",
        "field_dataset_name = \"od_test_after_deduplication\"\n",
        "threshold=200\n",
        "\n",
        "# create test_session object of TestSession instance\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = DriftDetectionRules()\n",
        "rules.add(type=\"single_class_anomaly_detection\", dist_metric=\"Mahalanobis\", _class=\"ALL\", threshold=threshold)\n",
        "\n",
        "#To Run OD Test\n",
        "edge_case_detection = data_drift_detection(test_session=test_session,\n",
        "                                           test_name=\"drift-test-1\",\n",
        "                                           train_dataset_name=train_dataset_name,\n",
        "                                           field_dataset_name=field_dataset_name,\n",
        "                                           train_embed_col_name=\"ImageVectorsM1\",\n",
        "                                           field_embed_col_name = \"ImageVectorsM1\",\n",
        "                                           level = \"image\",\n",
        "                                           rules = rules)\n",
        "\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "C5yxXLpXEW7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model A/B Test"
      ],
      "metadata": {
        "id": "ucZKALVBEdy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "\n",
        "rules = MCTRules()\n",
        "\n",
        "rules.add(metric='F1Score',\n",
        "          metric_threshold=0.6,\n",
        "          conf_threshold=0.1,\n",
        "          iou_threshold=0.2,\n",
        "          differenceThreshold=45,\n",
        "          clazz=[\"ALL\"])\n",
        "\n",
        "dataset_name = \"ab_test_object_detection_v0\"\n",
        "\n",
        "cls_default = clustering(test_session=test_session,\n",
        "                         dataset_name=dataset_name,\n",
        "                         method=\"k-means\",\n",
        "                         embedding_col=\"ImageVectorsM1\",\n",
        "                         level=\"image\",\n",
        "                         args={\"numOfClusters\": 5}\n",
        "                         )\n",
        "\n",
        "edge_case_detection = ab_test(test_session=test_session,\n",
        "                              dataset_name=dataset_name,\n",
        "                              test_name=\"ab_od_14121423\",\n",
        "                              modelAColumnName=\"Continental_Model\",\n",
        "                              modelBColumnName=\"YOLO_Model\",\n",
        "                              gtColumnName=\"AnnotationsV1\",\n",
        "                              rules=rules,\n",
        "                              outputType=\"labelled-od-cluster\",\n",
        "                              type=\"ab\",\n",
        "                              clustering=cls_default,\n",
        "                              embeddingColumnName=\"ImageVectorsM1\",\n",
        "                            #   aggregation_levels=[\"Reflection\",\n",
        "                            #                       \"CameraAngle\",\n",
        "                            #                       \"Overlap\"]\n",
        "                              )\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "zQ3CH9syEZ-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier Detection Test\n"
      ],
      "metadata": {
        "id": "PCX2I3GTElJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"roi_train_traffic_sign_detection_complete\"\n",
        "threshold=40\n",
        "\n",
        "# create test_session object of TestSession instance\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = DriftDetectionRules()\n",
        "rules.add(type=\"anomaly_detection\", dist_metric=\"Mahalanobis\", _class=\"ALL\", threshold=threshold)\n",
        "edge_case_detection = data_drift_detection(test_session=test_session,\n",
        "                                           test_name=f\"Outlier_Detection- train data for bbox\",\n",
        "                                           dataset_name=dataset_name,\n",
        "                                           embed_col_name = \"ImageVectorsM1\",\n",
        "                                           output_type = \"outlier_detection\",\n",
        "                                           rules = rules)\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "ykfVghb9Ek8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Outlier Detection (Depth Level)"
      ],
      "metadata": {
        "id": "C8St3-ICErPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"roi_train_outlier_df_depth_level2\"\n",
        "threshold=52\n",
        "\n",
        "# create test_session object of TestSession instance\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = DriftDetectionRules()\n",
        "rules.add(type=\"anomaly_detection\", dist_metric=\"Mahalanobis\", _class=\"ALL\", threshold=threshold)\n",
        "edge_case_detection = data_drift_detection(test_session=test_session,\n",
        "                                           test_name=f\"Outlier_Detection- train data for bbox at depth level 2\",\n",
        "                                           dataset_name=dataset_name,\n",
        "                                           embed_col_name = \"ImageVectorsM1\",\n",
        "                                           output_type = \"outlier_detection\",\n",
        "                                           rules = rules)\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "QGz7t-zTEvBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Failure Mode Analysis - Clustering\n"
      ],
      "metadata": {
        "id": "5ApcZm7kE1Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"object_detection_fma\"\n",
        "numOfClusters = 15\n",
        "conf_threshold = 0.1\n",
        "iou_threshold = 0.5\n",
        "run_name = \"Image detection test\"\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "\n",
        "rules = FMARules()\n",
        "rules.add(metric=\"Precision\", conf_threshold=conf_threshold, metric_threshold=0.75, iou_threshold=iou_threshold, label=\"ALL\")\n",
        "rules.add(metric=\"F1Score\", conf_threshold=conf_threshold, metric_threshold=0.75, iou_threshold=iou_threshold, label=\"ALL\")\n",
        "rules.add(metric=\"Recall\", conf_threshold=conf_threshold, metric_threshold=0.75, iou_threshold=iou_threshold, label=\"ALL\")\n",
        "\n",
        "cls_default = clustering(test_session=test_session,\n",
        "                         dataset_name = dataset_name,\n",
        "                         method=\"k-means\",\n",
        "                         embedding_col=\"ImageVectorsM1\",\n",
        "                         level=\"image\",\n",
        "                         args= {\"numOfClusters\": numOfClusters},\n",
        "                         force=True)\n",
        "\n",
        "edge_case_detection = failure_mode_analysis(test_session=test_session,\n",
        "dataset_name = dataset_name,\n",
        "test_name = f\"FMA-test-1\",\n",
        "model = \"ModelA\",\n",
        "gt = \"GT\",\n",
        "rules = rules,\n",
        "output_type=\"object_detection\",\n",
        "type=\"embedding\",\n",
        "clustering=cls_default)\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "sD8U8xr0E3-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Failure Mode Analysis - Metadata Test"
      ],
      "metadata": {
        "id": "OnS_fEuwE6lI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"object_detection_fma\"\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = FMARules()\n",
        "rules.add(metric=\"Precision\", conf_threshold=0.1, metric_threshold=0.7, iou_threshold=0.5, label=\"ALL\")\n",
        "rules.add(metric=\"Recall\", conf_threshold=0.1, metric_threshold=0.7, iou_threshold=0.5, label=\"ALL\")\n",
        "rules.add(metric=\"F1Score\", conf_threshold=0.1, metric_threshold=0.7, iou_threshold=0.5, label=\"ALL\")\n",
        "\n",
        "\n",
        "edge_case_detection = failure_mode_analysis(test_session=test_session,\n",
        "                                            dataset_name = dataset_name,\n",
        "                                            test_name = \"FMA-Meta-Test-1\",\n",
        "                                            model = \"ModelA\",\n",
        "                                            gt = \"GT\",\n",
        "                                            rules = rules,\n",
        "                                            output_type=\"object_detection\",\n",
        "                                            type=\"metadata\",\n",
        "                                            aggregation_level=['daytime','country','streetconditions','roadtype']\n",
        "                                            # aggregation_level=['roadtype']\n",
        "                                            )\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "BgZyYXqRE6TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Failure Mode Analysis - Metadata Test (Depth Level)\n"
      ],
      "metadata": {
        "id": "G-UhFX-7E_wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = FMARules()\n",
        "rules.add(metric=\"Precision\", conf_threshold=0.1, metric_threshold=0.95, iou_threshold=0.5, label=\"ALL\")\n",
        "rules.add(metric=\"Recall\", conf_threshold=0.1, metric_threshold=0.95, iou_threshold=0.5, label=\"ALL\")\n",
        "rules.add(metric=\"F1Score\", conf_threshold=0.1, metric_threshold=0.95, iou_threshold=0.5, label=\"ALL\")\n",
        "\n",
        "\n",
        "edge_case_detection = failure_mode_analysis(test_session=test_session,\n",
        "                                            dataset_name = \"fma_roi_v1\",\n",
        "                                            test_name = \"Test2\",\n",
        "                                            model = \"ModelA\",\n",
        "                                            gt = \"GT\",\n",
        "                                            rules = rules,\n",
        "                                            output_type=\"object_detection\",\n",
        "                                            type=\"metadata\",\n",
        "                                            aggregation_level=['depth_level']\n",
        "                                            )\n",
        "\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "mjoty5JDFDHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labeling Quality Test"
      ],
      "metadata": {
        "id": "0Cs9maYTFGQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"object_detection_al_lq_v2\"\n",
        "metric_threshold = 0.75\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "rules = LQRules()\n",
        "rules.add(metric=\"mistake_score\", label=[\"ALL\"], metric_threshold=metric_threshold)\n",
        "\n",
        "edge_case_detection = labelling_quality_test(\n",
        "    test_session=test_session,\n",
        "    dataset_name=dataset_name,\n",
        "    test_name=\"labelling-quality-test-2\",\n",
        "    type=\"labelling_consistency\",\n",
        "    output_type=\"object_detection\",\n",
        "    mistake_score_col_name=\"MistakeScore\",\n",
        "    gt=\"GT\",\n",
        "    model=\"GT\",\n",
        "    rules=rules,\n",
        ")\n",
        "test_session.add(edge_case_detection)\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "KSC7X1zNFGfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Image Property Drift Test"
      ],
      "metadata": {
        "id": "UDFrdBiPFRHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_dataset_name = \"od_train_after_deduplication\"\n",
        "eval_dataset_name = \"od_test_after_deduplication\"\n",
        "\n",
        "test_session = TestSession(\n",
        "    project_name=\"Continental-Projects\",\n",
        "    run_name=run_name,\n",
        "    access_key=ACCESS_KEY,\n",
        "    secret_key=SECRET_KEY,\n",
        "    host=HOST\n",
        ")\n",
        "\n",
        "\n",
        "rules = IPDRules()\n",
        "rules.add(metric=\"image-property-suite\")\n",
        "\n",
        "edge_case_detection = image_property_drift(test_session=test_session,\n",
        "                                           reference_dataset_name=reference_dataset_name,\n",
        "                                           eval_dataset_name=eval_dataset_name,\n",
        "                                           rules=rules,\n",
        "                                           test_name=\"Image-Property-Drift-test-1\",\n",
        "                                           type=\"image-property-drift\",\n",
        "                                           output_type=\"image-data\"\n",
        "                                           )\n",
        "test_session.add(edge_case_detection)\n",
        "\n",
        "test_session.run()"
      ],
      "metadata": {
        "id": "tyeHOVG9FQ60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cMINMo8XcnVQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}